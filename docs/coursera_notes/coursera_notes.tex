\documentclass[12pt]{article}
\usepackage{amssymb, amsmath}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{empheq}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor,cancel}

% Set up the margins.
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

% See
%   https://tex.stackexchange.com/questions/20609/strikeout-in-math-mode/20612
%
\newcommand\hcancel[2][black]{\setbox0=\hbox{$#2$}
\rlap{\raisebox{.45\ht0}{\textcolor{#1}{\rule{\wd0}{1pt}}}}#2}

\begin{document}


\section{Probabilistic Graphical Models 1: Representation}

\subsection{Week 1. Introduction and Overview}

Please see the student example in Fig.~\ref{fig:student_example}.

\begin{figure}[H]
\centering
\includegraphics[width=6.5in]{graphics/example_student.png}
\caption{Student Example. Each node is annotated with a conditional probability distribution (CPD).}
\label{fig:example_student}
\end{figure}


\begin{itemize}
    \item Grade ($G$) --- $g^1(A)$, $g^2(B)$, and $g^3(C)$.
    \item Course Difficulty ($D$) --- $d^0(\text{easy})$, $d^1(\text{hard})$.
    \item Student Intelligence ($I$) --- $i^0(\text{low})$ and $i^1(\text{high})$.
    \item SAT score ($S$).
    \item Reference Letter ($L$).
\end{itemize}
 

% D, I --> G --> L
% I --> S


\subsubsection{Joint Distribution}
Consider $\Pr(I, D, G)$.

\subsubsection{Conditioning}
For example, we can condition on $G=g^1$.  We look at $\Pr(I, D, g^1)$.
We need to renormalize. That is
\begin{equation*}
  \Pr(I, D | g^1) = \frac{\Pr(I, D, g^1)}
                         {\sum_{I=i^j} \sum_{D=d^k} \Pr(I, D, g^1)}  \, .
\end{equation*}
The above equation comes from the definition of conditional probability $\Pr(A | B) = \Pr(A, B) / \Pr(B)$.

\subsubsection{Marginalization}
For example, we marginalize $I$.

\begin{equation*}
  \Pr(D) = \sum_{I=i^j} \Pr(I, D)
\end{equation*}


\subsubsection{Factors}
A factor $\phi$ maps random variables to a real number. That is
\begin{equation*}
  \phi : (X_1, \ldots, X_k) \rightarrow \Re
\end{equation*}
A joint distribution $\Pr(I, D, G)$ is a factor.

We compute a factor product using
\begin{equation*}
  \phi(A=a^i, B=b^j, C=c^k) = \phi_1(A=a^i, B=b^j) \phi_2(B=b^j, C=c^k) \, .
\end{equation*}

We can marginalize a factor too.
\begin{equation*}
  \phi(A=a^i, C=c^k) = \sum_j \phi(A=a^i, B=b^j, C=c^k) \,.
\end{equation*}




\subsection{Week 1. Bayesian Network (Directed Models)}

A Bayesian network is a directed acyclic graph (DAG) whose nodes represent the random variables $X_1, \ldots, X_n$.
\subsubsection{Factorization}
Chain rule:
\begin{equation}
  \Pr(D, I, G, S, L) = \Pr(D) \Pr(I) \Pr(G|I, D) \Pr(S|I) \Pr(L|G)
\end{equation}

\begin{equation}
  \Pr(X_1, ..., X_n) = \prod_{i=1}^n \Pr(X_i | \text{Par}_\mathbb{G}( X_i ) ) \, ,
\end{equation}
where $\text{Par}_\mathbb{G}( X_i )$ are the parents of $X_i$ over the graph $\mathbb{G}$.

% ---

\subsubsection{Reasoning Patterns}
\paragraph{Causal reasoning}
$\Pr(L = l^1 | I = i^0) = 0.39$.

Given an easier class, $\Pr(L = l^1 | I = i^0, D = d^0) = 0.51$.

\paragraph{Evidential reasoning (from the bottom up)}
$\Pr(D = d^1 | G = g^3) = 0.63$.

Pay attention to the details in the calculations. We must use Bayes' theorem.

\begin{align*}
\Pr(d^1 | g^3) & = \frac{\Pr(d^1, g^3)}
                      {\Pr(g^3)} \\
               & = \frac{\Pr(d^1, g^3, i^0) + \Pr(d^1, g^3, i^1)}
                        {\Pr(g^3 | i^0, d^0) \Pr(i^0) \Pr(d^0) + \Pr(g^3 | i^0, d^1) \Pr(i^0) \Pr(d^1) + \ldots} \\
               & = \frac{\Pr(g^3 | i^0, d^1) \Pr(i^0) \Pr(d^1) + \Pr(g^3 | i^1, d^1) \Pr(i^1) \Pr(d^1)}
                        {\Pr(g^3 | i^0, d^0) \Pr(i^0) \Pr(d^0) + \Pr(g^3 | i^0, d^1) \Pr(i^0) \Pr(d^1) + \ldots} \, .
\end{align*}



\paragraph{Intercausal reasoning}

The probability that the student is highly intelligent is $\Pr(I = i^1) = 0.3$.

Given that the student got a B, $\Pr(I = i^1 | G = g^2) = 0.175$.

Given that he/she got a B and the class was difficult, $\Pr(I = i^1 | G = g^2, D = d^1) = 0.34$.


As another example, let $Y = (X_1 \text{or} X_2)$.

$\Pr(X_2 = 1 | Y = 1) = 2/3$, but $\Pr(X_2 = 1 | Y = 1, X_1 = 1) = 1/2$.


\subsubsection{Flow of Probabilistic Influence}
\paragraph{When can $X$ influence $Y$?} In other words, does conditioning on $X$ change our beliefs about $Y$?

\begin{itemize}
  \item $X \rightarrow Y$. $X$ is the parent of $Y$.

  \item $X \leftarrow Y$. $X$ is the child of $Y$.

  \item $X \rightarrow W \rightarrow Y$.

  \item $X \leftarrow W \leftarrow Y$.

  \item $X \leftarrow W \rightarrow Y$. $W$ is the common parent.

  \item $\hcancel[red]{X \rightarrow W \leftarrow Y}$. $W$ is the common child.

\end{itemize}

\paragraph{When can $X$ influence $Y$ given evidence about $Z$?}

\begin{center}
\begin{tabular}{|c|c|c|}
    \hline
    Scenario & \multicolumn{2}{|c|}{Can $X$ influence $Y$ given evidence about $Z$?} \\ \hline
     & $W \not\in Z$ & $W \in Z$  \\ \hline

    $X \rightarrow Y$ & \multicolumn{2}{|c|}{Yes} \\

    $X \leftarrow Y$ & \multicolumn{2}{|c|}{Yes} \\

    $X \rightarrow W \rightarrow Y$ & Yes & No \\

    $X \leftarrow W \leftarrow Y$ & Yes & No \\

    $X \leftarrow W \rightarrow Y$ & Yes & No \\

    $X \rightarrow W \leftarrow Y$ & No if $W$ and all its descendants $\not\in Z$ & Yes if $W$ or one of its descendants $\in Z$\\
    \hline
  \end{tabular}
\end{center}

\paragraph{Active Trails}
A trail $X_1 - \ldots - X_n$ is active given $Z$ if
\begin{enumerate}
\item for any v-structure $X_{i-1} \rightarrow X_i \leftarrow X_{i+1}$ we have $X_i$ or one of its descendants in $Z$, and

\item no other $X_i$ is in $Z$.

\end{enumerate}


\subsubsection{Independence}
For events $\alpha$ and $\beta$, $\Pr \vdash \alpha \bot \beta$ if

\begin{itemize}
  \item $\Pr(\alpha, \beta) = \Pr(\alpha) \Pr(\beta)$, or

  \item $\Pr(\alpha | \beta) = \Pr(\alpha$), or

  \item $\Pr(\beta | \alpha) = \Pr(\beta)$
\end{itemize}

From Wikipedia, $P \vdash Q$ means ``from $P$, I know that $Q$''.

\subsubsection{Conditional independence}
For sets of random variables $X$, $Y$, and $Z$, $\Pr \vdash (X \bot Y | Z)$ if

\begin{itemize}
  \item $\Pr(X, Y|Z) = \Pr(X|Z) \Pr(Y|Z)$, or

  \item $\Pr(X|Y, Z) = \Pr(X|Z)$, or

  \item $\Pr(Y|X, Z) = \Pr(Y|Z)$, or

  \item $\Pr(X, Y, Z) \propto \phi_1(X, Z) \phi_2(Y, Z) $
\end{itemize}



\paragraph{Example 1}  Tossing a coin twice, which might be fair or not. The two outcomes (from the two tosses) are $X_1$ and $X_2$.

\begin{figure}[H]
\centering
\includegraphics[width=2in]{graphics/example_coin.png}
\caption{A fair or biased coin is tossed twice. $X_1$ and $X_2$ are the two outcomes, respectively.}
\label{fig:example_coin}
\end{figure}

% C --> X_1, X_2.

$\Pr(X_2 = \text{head} | X_1 = \text{head}) > 0.5$ because the coin might not be fair.
However, $\Pr(X_2 = \text{head}| C = \text{fair coin})$ is independent of $X_1$!

Also $\Pr(X_2 = \text{head}| C = \text{biased coin})$ is independent of $X_1$. That is, once we know what the coin is, the two outcomes are no longer correlated.

In her notation,
\begin{itemize}
  \item $\Pr$ does NOT satisfy $X_1 \bot X_2$
  \item $\Pr$ satisfies $(X_1 \bot X_2 | C)$.
\end{itemize}
Conditioning can remove independence.


\paragraph{Example 2} 
We have,
\begin{equation*}
I \rightarrow G, S
\end{equation*}

\begin{equation*}
  \Pr(S, G | I=i^0) = \Pr(S | I=i^0) \Pr(G | I=i^0) \, .
\end{equation*}



\paragraph{Example 3} However, conditioning can also introduce independence.
We have,
\begin{equation*}
I, D \rightarrow G
\end{equation*}

Look at $\Pr(I, D| G = 1)$.  It couples $I$ and $D$.


\subsubsection{Independencies in Bayesian Networks}
$X$ and $Y$ are $d$-separated if $\mathbb{G}$ given $Z$,
$\text{d-sep}_{\mathbb{G}}(X, Y|Z)$,
if there is no active trail in $\mathbb{G}$ between $X$
and $Y$ given $Z$.

If $\Pr$ factorizes over $\mathbb{G}$, and $d-\text{sep}_{\mathbb{G}}(X, Y|Z)$, then $\Pr$ satisfies $(X \bot Y | Z)$.
In other words,
\begin{equation*}
  \text{factorization} \rightarrow \text{independence}
\end{equation*}

Any node is $d$-separated from its non-descendants given its parents.
In other words, if $\Pr$ factorizes over $\mathbb{G}$, then in $\Pr$,
any variable is independent of its non-descendants given its parents.


\paragraph{$I$-map (Independency Map)}

\begin{equation*}
  I(\mathbb{G}) = \{ (X \bot Y | Z) : d-\text{sep}_{\mathbb{G}}(X, Y|Z) \}
\end{equation*}

If $\Pr$ satisfies $I(\mathbb{G})$, $\mathbb{G}$ is an $I$-map of $\Pr$.

$\Pr$ factorizes over $\mathbb{G} \leftrightarrow \mathbb{G}$ is an $I$-map for $\Pr$.

\paragraph{Quiz}

$I$-maps can also be defined directly on graphs as follows. Let $I(\mathbb{G})$ be the set of independencies encoded by a graph $\mathbb{G}$. Then $\mathbb{G}_1$ is an $I$-map for $\mathbb{G}_2$ if $I(\mathbb{G}_1) \subseteq I(\mathbb{G}_2)$.

A graph $\mathbb{K}$ is an $I$-map for a graph $\mathbb{G}$ if and only if all of the independencies encoded by $\mathbb{K}$ are also encoded by $\mathbb{G}$.

\subsubsection{Naive Bayes}

Class $C \rightarrow$ Features $X_1, \ldots, X_n$.

Assume that $(X_i \bot X_j | C)$ for all $X_i, X_j$.
\begin{equation*}
  \Pr(C, X_1, \ldots, X_n) = \Pr(C) \sum_{i=1}^n \Pr(X_i | C)
\end{equation*}



\subsection{Week 2. Template Models for Bayesian Networks}

\subsubsection{Temporal Models}

Using the chain rule for probability,
\begin{equation*}
  \Pr(X^{(0:T)}) = \Pr(X^{(0)}) \prod_{t=0}^{T-1} \Pr\left(X^{(t+1)} \middle\vert X^{(0:t)} \right) \, .
\end{equation*}

If we make the Markov assumption where the system has no memory, $\left( X^{(t+1)} \bot X^{(0:t-1)} \middle\vert X^{(t)}\right)$.
We have
\begin{equation*}
  \Pr(X^{(0:T)}) = \Pr(X^{(0)}) \prod_{t=0}^{T-1} \Pr\left(X^{(t+1)} \middle\vert X^{(t)} \right) \, .
\end{equation*}

We can further assume that the probability model is time-invariant.
\begin{equation*}
  \Pr\left(X^{(t+1)} \middle\vert X^{(t)} \right) = \Pr\left(X' \middle\vert X \right) \, .
\end{equation*}

In the traffic model shown in Fig.~\ref{fig:example_template_transit_model},
\begin{equation*}
  \Pr(W', V', L', F', O' | W, V, L, F) = \Pr(W'|W) \Pr(V'|W, V) \Pr(L' | L, V) \Pr(F' | F, W) \Pr(O' | L', F') \, .
\end{equation*}
Note that $O$ is not referenced as it does not affect anything later in time.

\begin{figure}[H]
\centering
\includegraphics[width=6.5in]{graphics/example_template_transit_model.png}
\caption{A Traffic Model.}
\label{fig:example_template_transit_model}
\end{figure}

\subsection{Week 2. Structured CPDs for Bayesian Networks}

\subsubsection{Context-Specific Independence}
\begin{equation*}
  \Pr \vdash (X \bot_c Y | Z, c) \, ,
\end{equation*}
where $c$ is an assignment of some random variable $C$.

\begin{figure}[H]
\centering
\includegraphics[width=5.5in]{graphics/example_context_specific_independence.png}
\caption{A Sample Question on Context-Specific Independence.  The answers are False, True, True, and False.}
\label{fig:example_context_specific_independence}
\end{figure}

\subsubsection{Tree-Structured CPDs}

\begin{figure}[H]
\centering
\includegraphics[width=6in]{graphics/example_tree_structured_cpds.png}
\caption{A Sample Question on Tree-Structured CPDs.  The answers are True, False, True, and True.}
\label{fig:example_tree_structured_cpds}
\end{figure}

\paragraph{Multiplexer CPD}

In a multiplexer CPD, $A$ is a selector that decides which of the $Z_i$'s will be copied to $Y$.
\begin{equation*}
  \Pr(Y | A, Z_1, \ldots, Z_k) = 
    \begin{cases}
    1 & \text{if } Y = Z_A \\
    0 & \text{otherwise}
    \end{cases}
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=3in]{graphics/multiplexer_cpd.png}
\caption{The Multiplexer CPD.}
\label{fig:multiplexer_cpd}
\end{figure}


\subsubsection{Independence of Causal Influence}

\paragraph{Noise OR CPD}
\begin{align*}
  \Pr(Z_i | X_i) & = 
    \begin{cases}
      0 & \text{if } X_i = 0 \\
      \lambda_i & \text{if } X_i = 1
    \end{cases} \\
  \Pr(Y=0 | X_1, \ldots, X_k) & = (1 - \lambda_0) \prod_{i: X_i = 1} (1 - \lambda_i) \, . \\
  \Pr(Y=1 | X_1, \ldots, X_k) & = 1 - \Pr(Y=0 | X_1, \ldots, X_k) \, .
\end{align*}

In other words, $\Pr(Z_i=0 | X_i=1) = 1 - \lambda_i$.

\begin{figure}[H]
\centering
\includegraphics[width=3.5in]{graphics/noisy_or_cpd.png}
\caption{A Noisy OR CPD.}
\label{fig:noisy_or_cpd}
\end{figure}


\paragraph{Sigmoid CPD}

\begin{align*}
  Z & = w_0 + \sum_{i=1}^k w_i X_i \, .\\
  \Pr(y^1 | X_1, \ldots, X_k) & = \text{sigmoid}(Z) \, .
\end{align*}

\begin{figure}[H]
\centering
\includegraphics[width=3in]{graphics/sigmoid_cpd.png}
\caption{A Sigmoid CPD.}
\label{fig:sigmoid_cpd}
\end{figure}


\subsubsection{Continuous Variables}

\paragraph{Temperature Example}
\begin{align*}
  S & \sim \mathcal{N}(T, \sigma^2) \\
  T' & \sim 
    \begin{cases}
      \mathcal{N}(\alpha_0 T + (1-\alpha_0)O, \sigma_{0T}^2) & \text{if } D = 0 \\
      \mathcal{N}(\alpha_1 T + (1-\alpha_1)O, \sigma_{1T}^2) & \text{if } D = 1
    \end{cases}
\end{align*}

\paragraph{Conditional Linear Gaussian}

\begin{equation*}
  Y \sim \mathcal{N} \left( w_{a0} + \sum_i w_{ai} X_i, \sigma_a^2 \right) \, .
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=3in]{graphics/conditional_linear_gaussian.png}
\caption{A Conditional Linear Guassian.}
\label{fig:conditional_linear_gaussian}
\end{figure}


\subsection{Week 3. Markov Networks (Undirected Models)}


\subsection{Week 4. Decision Making}


\subsection{Week 5. Knowledge Engineering \& Summary}


\end{document}
